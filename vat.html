<!DOCTYPE HTML>
<!--
	Alpha by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Naomi, UX Researcher</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<header id="header">
					<h1><a href="index.html">Naomi Johnson</a></h1>
					<nav id="nav">
						<ul>
							<li>
								<a href="#" class="icon solid fa-angle-down">Projects</a>
								<ul>
									<li><a href="mentor.html">Mentor - search & request</a></li>
									<li><a href="jdict.html">Jdict - language tools</a></li>
									<li><a href="poirot.html">Poirot - dev tools</a></li>
									<li><a href="juxxt.html">Juxxt - design tools</a></li>
									<li><a href="vat.html">VAT - annotation tools</a></li>
								</ul>
							</li>
						</ul>
					</nav>
				</header>

			<!-- Main -->
				<section id="main" class="container">
					<header>
						<h2>Video Annotation Tool (VAT)</h2>
						<p>Simplifying data labeling for machine learning</p>
					</header>
					<div class="box">
						<h3>Summary</h3>
						<p>Data from gyroscopes and accelerometers often has multiple axises and can be difficult to read, but having a video that shows the sensor does not always make annotators more accurate or efficient. I designed a study to investigate why this might be the case, ran 60 users through the study, and wrote scripts to analyze quantitive data about how users used our Video Annotation Tool (VAT) and their annotations.
						<p>Timeline: November 2016-April 2018</p>
						<p>Team: Lawrence Thatcher, Dr. Michael Jones, and Dr. Kevin Seppi</p>
						<h3>The problem</h3>
						<p>Training classifiers for human activity recognition systems often relies on large corpora of annotated sensor data; but collecting this data can be expensive both in terms of time and money.
							This captioned video below introduces our Video Annotation Tool which lets users gather data and annotate it so they can create prototypes. </p>
						<center>
		    			<iframe width="95%" height="400" src="https://www.youtube.com/embed/bhVvr1DPUDA" frameborder="0" allowfullscreen ng-show="showvideo"></iframe>
		    			</center>
		    			<br>
		    			<p>If your browser doesn't permit embeded Youtube videos, here's a <a href="https://www.youtube.com/watch?v=bhVvr1DPUDA">link</a> for you.</p>
		    			<br>
						<h3>Previous research</h3>
						<p>My team anticipated that allowing users to see video and data together would make users more accurate and efficient annotators, but we found that having video did not always make users more accurate (I am second author on our <a href="https://dl.acm.org/doi/abs/10.1145/3267305.3267507">UbiComp '18 paper</a> and first author on our chapter in the book <a href="https://link.springer.com/chapter/10.1007/978-3-030-13001-5_7">Human Activity Sensing</a>). Through this research, we formed two hypothesis:</p>
						<ol>
							<li>Video seems helpful in allowing one to discover patterns in the data which correspond to activity events. We refer to this as a “Rosetta Stone” effect.</li>
							<li>Data is useful in moving quickly through the events to find event boundaries. We refer to this as “indexing".</li>
						</ol>
						<p>I designed a user study, analyzed data, and drafted a paper that explored these two hypothesis. As of June 2020, our paper is currently in revision. </p>
						<span class="image featured"><img src="images/vat/vat_gui.png" alt=""/></span>
						<h3>User studies</h3>
						<!-- <p>For this project I designed, ran, and evaluated data from an IRB-approved study to explore whether users could collect video and data, as well as produce annotations suitable for machine learning.</p> -->
						<p>I measured efficiency by giving participants a fixed amount of time in which to do their work, which reduced the length of the study and allowed us to compare completion rate rather than time tocompletion. Enforcing a time limit prevented us from seeing decreased efficiency due to fatigue. I also measured accuracy, which I split into two parts: correctly identifying the event type and correctly identifying the bounds (beginning and ending) of an event. I measured the accuracy of event boundary annotations using an asynchronous continuous time model, rather than Krippendorff’s alpha (a metric which is artificially inflated if the sensor data include long periods of time in which nothing happens, or long periods during which a single event happens, and the annotatorscorrectly label those long time periods). I was more interested in how annotators handled the boundaries at the start and end of an event.</p>
						<p>I calculated a Z score to determine that we would need 60 partcipants, and designed the study so that 15 people could participate at once (the size of the computer lab I had access to). Participants were recruited from on-campus advertisements. All participants received $15 US for their participation. I told participants that the top 1/3 would of performers would receive an additional $40 US, so participants were highly motivated to be as accurate and efficient as possible in that window of time. I told participants that they did not need to find the exact boundary, but that a small margin of error (less than 0.5 seconds) was acceptable.</p>
						<p>I began by showing participants a three minute training video about the purpose of annotating data for machine learning algorithms in order to create prototypes and then a three minute video introducing them to the VAT interface. Each participant performed 4 annotation tasks with different interfaces. The datasets and interface types were counterbalanced, as shown in the table below. In each task, participants used the interface with both video and data first, followed by either the data only or video only interface, and concluding with the third interface type. After completing each task, we asked users to complete a three question survey.</p>
						</ol>
						<div class="table-wrapper">
							<table class="alt">
								<thead>
									<tr>
										<th></th>
										<th>Group A</th>
										<th>Group B</th>
										<th>Group C</th>
										<th>Group D</th>
									</tr>
								</thead>
								<tbody>
									<tr class="highlight">
										<th>Practice Task</th>
										<td>Cane</td>
										<td>Cane</td>
										<td>Cane</td>
										<td>Cane</td>
									</tr>
									<tr>
										<th></th>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
									</tr>
									<tr class="highlight">
										<th>First Task</th>
										<td>Run</td>
										<td>Run</td>
										<td>Run</td>
										<td>Run</td>
									</tr>
									<tr>
										<th></th>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
									</tr>
									<tr class="highlight">
										<th>Second Task</th>
										<td>Pills</td>
										<td>Pills</td>
										<td>Pills</td>
										<td>Pills</td>
									</tr>
									<tr>
										<th></th>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
										<td>Both</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Data</td>
										<td>Only Video</td>
									</tr>
									<tr class="highlight">
										<th>Third Task</th>
										<td>Run</td>
										<td>Run</td>
										<td>Pills</td>
										<td>Pills</td>
									</tr>
									<tr>
										<th></th>
										<td>Only Data</td>
										<td>Only Data</td>
										<td>Only Video</td>
										<td>Only Video</td>
									</tr>
								</tbody>
							</table>
						</div>
						<p>My collaborator added backend features to our interface to log all interface events generated by each participant. We divided the events into “data” events which involved the data interface, “video” events which involved only the video interface and “neutral” events which correspond to interface elements which are visible in every interface type.</p>
						<h3>Analysis</h3>
						<p>We categorized users' annotations into four types:</p>
						<div class="table-wrapper">
							<table class="alt">
								<!-- <thead>
									<tr>
										<th></th>
										<th>Group A</th>
									</tr>
								</thead> -->
								<tbody>
									<tr class="highlight">
										<td>Name</td>
										<td>Definition</td>
									</tr>
									<tr>
										<td>Missed</td>
										<td>Participant did not add bounds within the event boundaries.</td>
									</tr>
									<tr>
										<td>type &¬bounds</td>
										<td>Type label is correct, but one or more bounds did not fall within our epsilon.</td>
									</tr>
									<tr>
										<td>¬type &¬bounds</td>
										<td>Type label does not match and one or more bounds did not fall within our epsilon.</td>
									</tr>
									<tr>
										<td>¬type & bounds</td>
										<td>Type label type does not match the event type but both bounds are correct.</td>
									</tr>
									<tr>
										<td>type & bounds</td>
										<td>Type label is correct and both bounds fall within our prescribed epsilon.</td>
									</tr>
								</tbody>
							</table>
						</div>
						<p>We found that: </p>
							<ol>
								<li>annotators worked more efficiently, attempted to annotate more events and annotated more events correctly using the data interface compared to the video interface.</li>
								<li>Different kinds of errors were made depending on the type of data being analyzed and the interface being used.</li>
								<li>For the pills dataset, participants annotated events correctly more often using the datainterface than the video interface. For the running dataset, participants annotated events correctly with about the samefrequency. The frequency of annotating either the event type or bounds incorrectly is different across interface type. </li>
								<li> After participants had completed several rounds of annotation with each interface type, we asked them to ratethe importance of each interface element for time points in a hypothetical future annotation task. Participants reported that data and video were both "very" essential for their first time annotating; after several rounds of annotating video was "not really" essential while data was still "very" essential.</li>
								<li>When both the data and video interfaces are available, participants use data and video controls to different extents. Of the controls used, scrubbing the data control is is the most used.</li>
							</ol>
						<h3>Impact</h3>
						<p>Our study has several implications for D+V annotation tasks in the context of supervised learning of sensor-based activity recognizers. First, it may not be essential to collect video of all events for use during annotation. Collecting some video for training is important, but once users have learned to spot patterns, they appear be more accurate and more efficient without the video. This means that projects in which it is unfeasible to collect video paired with all sensor data may generate enough video to successfully train annotators. Second, users may need to start with both video and data together to learn to spot events in the data using the video as reference. This suggests that future tools for D+V annotation might provide an easy way, or an adaptive automatic way, to switch from a mode involving both data and video to a mode involving data only. Third, it may be useful to more deeply explore the design of “data only” interfaces for annotating events. These interfaces might show examples of the data trace of certain event types for reference or make it easier to zoom in and out of the temporal scale of an event trace. Fourth, users are somewhat less accurate when labeling event types without video. Investigating machine learning algorithms which are less sensitive to a few mislabeled event types in a larger corpus of labeled data compared to a perfectly labeled smaller corpus may improve the overall accuracy of an activity recognizer.</p>
					</div>
				</section>

				<!-- Footer -->
				<section id="footer">
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/naomi789/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://www.github.com/naomi789" class="icon brands fa-github"><span class="label">Github</span></a></li>
						<!-- <li><a href="https://www.instagram.com/naomi.johndaughter/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li> -->
						<!-- <li><a href="https://www.naomijohndaughter.com" class="icon brands fa-camera"><span class="label">Photography</span></a></li> -->
						<li><a href="#snjohnson789@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
					</ul>
					<p class="copyright">&copy; 2020 <a href="index.html">Naomi Johnson</a>.
	  					Design: <a href="http://html5up.net">HTML5 UP</a> <a href="https://html5up.net/alpha">Alpha</a>.
					</p>
				</section>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
